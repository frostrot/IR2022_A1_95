{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processing\n",
    "\n",
    "We load the index from the stored pickle file first for the querying purposes. Then, using the positional index established, we build pointers for all of the words in the query and try to discover documents that include all of the query's terms in the right sequence.\n",
    "\n",
    "**Input Format**    \n",
    "\n",
    "Input statement for querying of length <=5\n",
    "\n",
    "**Output Format**  \n",
    "\n",
    "Number of documents retrieved: Returned value \\\n",
    "List of Documents retrieved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/frostrot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing neccesary files\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import pickle as pkl\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from ipynb.fs.defs.data_preprocessing import remove_punc\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pickle files\n",
    "\n",
    "with open(\"./pickle_files/index.pkl\",\"rb\") as f:\n",
    "    index = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords, and convert shorted words into there extended forms\n",
    "\n",
    "def stopword(x):\n",
    "  EXTENDED_FORMS = {\"aren't\": 'are not', \"can't\": 'cannot', \"couldn't\": 'could not', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'll\": 'he will', \"he's\": 'he is', \"i'd\": 'i would', \"i'll\": 'i will', \"i'm\": 'i am', \"isn't\": 'is not', \"it's\": 'it is', \"it'll\": 'it will', \"i've\": 'i have', \"let's\": 'let us', \"mightn't\": 'might not', \"mustn't\": 'must not',\"n't\": 'not', \"shan't\": 'shall not', \"she'd\": 'she would', \"she'll\": 'she will', \"she's\": 'she is', \"shouldn't\": 'should not', \"that's\": 'that is', \"there's\": 'there is', \"they'd\": 'they would', \"they'll\": 'they will', \"they're\": 'they are', \"they've\": 'they have', \"we'd\": 'we would', \"we're\": 'we are', \"weren't\": 'were not', \"we've\": 'we have', \"what'll\": 'what will', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"where's\": 'where is', \"who'd\": 'who would', \"who'll\": 'who will', \"who're\": 'who are', \"who's\": 'who is', \"who've\": 'who have', \"won't\": 'will not', \"wouldn't\": 'would not', \"you'd\": 'you would', \"you'll\": 'you will', \"you're\": 'you are', \"you've\": 'you have', \"'re\": ' are', \"wasn't\": 'was not', \"we'll\": 'we will', \"'cause\": 'because', \"could've\": 'could have', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"i'd've\": 'i would have', \"i'll've\": 'i will have', \"it'd\": 'it would', \"it'd've\": 'it would have', \"it'll've\": 'it will have', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd've\": 'she would have', \"she'll've\": 'she will have', \"should've\": 'should have', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so as', \"this's\": 'this is', \"that'd\": 'that would', \"that'd've\": 'that would have', \"there'd\": 'there would', \"there'd've\": 'there would have', \"here's\": 'here is', \"they'd've\": 'they would have', \"they'll've\": 'they will have', \"to've\": 'to have', \"we'd've\": 'we would have', \"we'll've\": 'we will have', \"what'll've\": 'what will have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where've\": 'where have', \"who'll've\": 'who will have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd've\": 'you would have', \"you'll've\": 'you will have'}\n",
    "  \n",
    "  for i in range(len(x)):\n",
    "    if x[i] in EXTENDED_FORMS:\n",
    "      x[i] = EXTENDED_FORMS[x[i]]\n",
    "    if x[i] in stopwords.words('english'):\n",
    "      x[i]=''\n",
    "  x=remove_punc(x)\n",
    "  return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the parsed text, by, converting them into lowercase, removing any tags, extra spaces.\n",
    "\n",
    "def filter(item):\n",
    "  if type(item)==str:\n",
    "    item=item.lower()\n",
    "    words = word_tokenize(item)\n",
    "    item=stopword(words)\n",
    "    item=re.sub(r'\\\\N','',item)\n",
    "  return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional query processing\n",
    "\n",
    "def positional_query(text):\n",
    "    sentence_words = filter(text).split()\n",
    "    n = len(sentence_words)\n",
    "    print(sentence_words)\n",
    "    if n<1:\n",
    "        print(\"Sentence empty after filtering\")\n",
    "        return []\n",
    "        \n",
    "    pointers = [0 for i in range(n)]\n",
    "    documents = []\n",
    "    flag = True\n",
    "    \n",
    "    for i in range(n):\n",
    "        if sentence_words[i] not in index:\n",
    "            flag = False\n",
    "            break\n",
    "            \n",
    "    while flag:\n",
    "        for i in range(n):\n",
    "            if pointers[i] == len(index[sentence_words[i]]):\n",
    "                flag = False\n",
    "                break\n",
    "                \n",
    "        if flag == False:\n",
    "            break\n",
    "\n",
    "        for i in range(1,n):\n",
    "            if index[sentence_words[i]][pointers[i]][0] != index[sentence_words[0]][pointers[0]][0]:\n",
    "                flag = False\n",
    "                break\n",
    "        \n",
    "        if flag:\n",
    "            for i in range(1,n):\n",
    "                if index[sentence_words[i]][pointers[i]][1] - index[sentence_words[i-1]][pointers[i-1]][1] != 1:\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                documents.append(index[sentence_words[0]][pointers[0]][0])\n",
    "        \n",
    "        j = 0\n",
    "        for i in range(1,n):\n",
    "            if index[sentence_words[j]][pointers[j]] > index[sentence_words[i]][pointers[i]]:\n",
    "                j = i\n",
    "        pointers[j] += 1\n",
    "        flag = True\n",
    "\n",
    "    unique = {}\n",
    "    for i in documents:\n",
    "        unique[i] = 1\n",
    "    return list(unique.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is me\n",
      "[]\n",
      "Sentence empty after filtering\n",
      "Number Of Documents Retrieved:  0\n",
      "List Of Documents Retrieved: \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter the Sentence to Query: \").strip()\n",
    "print(query)\n",
    "documents = positional_query(query)\n",
    "print('Number Of Documents Retrieved: ',len(documents))\n",
    "print('List Of Documents Retrieved: \\n',[i for i in documents])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1cbb85fbb159a91b512923319bfb5d4c6de1269d10ca612a23b6e07a28361c8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
