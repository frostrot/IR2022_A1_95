{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/frostrot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing neccesary files\n",
    "\n",
    "import nltk\n",
    "import io\n",
    "import re\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "(i) Convert the text to lower case \\\n",
    "(ii) Perform word tokenization \\\n",
    "(iii) Remove stopwords from tokens \\\n",
    "(iv) Remove punctuation marks from tokens \\\n",
    "(v) Remove blank space tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuations from the text\n",
    "\n",
    "def remove_punc(tokens):\n",
    "  table = string.punctuation\n",
    "  ptokens = []\n",
    "  for w in tokens:\n",
    "    if w not in table:\n",
    "      ptokens.append(w)\n",
    "  ptokens = [s for s in ptokens if s]\n",
    "  ptokens = [re.sub(r\"[\\n\\t]+\",\" \",s) for s in ptokens]\n",
    "  return ptokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords, and convert shorted words into there extended forms\n",
    "\n",
    "def stopword(x):\n",
    "  EXTENDED_FORMS = {\"aren't\": 'are not', \"can't\": 'cannot', \"couldn't\": 'could not', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'll\": 'he will', \"he's\": 'he is', \"i'd\": 'i would', \"i'll\": 'i will', \"i'm\": 'i am', \"isn't\": 'is not', \"it's\": 'it is', \"it'll\": 'it will', \"i've\": 'i have', \"let's\": 'let us', \"mightn't\": 'might not', \"mustn't\": 'must not',\"n't\": 'not', \"shan't\": 'shall not', \"she'd\": 'she would', \"she'll\": 'she will', \"she's\": 'she is', \"shouldn't\": 'should not', \"that's\": 'that is', \"there's\": 'there is', \"they'd\": 'they would', \"they'll\": 'they will', \"they're\": 'they are', \"they've\": 'they have', \"we'd\": 'we would', \"we're\": 'we are', \"weren't\": 'were not', \"we've\": 'we have', \"what'll\": 'what will', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"where's\": 'where is', \"who'd\": 'who would', \"who'll\": 'who will', \"who're\": 'who are', \"who's\": 'who is', \"who've\": 'who have', \"won't\": 'will not', \"wouldn't\": 'would not', \"you'd\": 'you would', \"you'll\": 'you will', \"you're\": 'you are', \"you've\": 'you have', \"'re\": ' are', \"wasn't\": 'was not', \"we'll\": 'we will', \"'cause\": 'because', \"could've\": 'could have', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"i'd've\": 'i would have', \"i'll've\": 'i will have', \"it'd\": 'it would', \"it'd've\": 'it would have', \"it'll've\": 'it will have', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd've\": 'she would have', \"she'll've\": 'she will have', \"should've\": 'should have', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so as', \"this's\": 'this is', \"that'd\": 'that would', \"that'd've\": 'that would have', \"there'd\": 'there would', \"there'd've\": 'there would have', \"here's\": 'here is', \"they'd've\": 'they would have', \"they'll've\": 'they will have', \"to've\": 'to have', \"we'd've\": 'we would have', \"we'll've\": 'we will have', \"what'll've\": 'what will have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where've\": 'where have', \"who'll've\": 'who will have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd've\": 'you would have', \"you'll've\": 'you will have'}\n",
    "  \n",
    "  for i in range(len(x)):\n",
    "    if x[i] in EXTENDED_FORMS:\n",
    "      x[i] = EXTENDED_FORMS[x[i]]\n",
    "    if x[i] in stopwords.words('english'):\n",
    "      x[i]=''\n",
    "  x=remove_punc(x)\n",
    "  return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the parsed text, by, converting them into lowercase, removing any tags, extra spaces.\n",
    "\n",
    "def filter(item):\n",
    "  if type(item)==str:\n",
    "    item=item.lower()\n",
    "    words = word_tokenize(item)\n",
    "    item=stopword(words)\n",
    "    item=re.sub(r'\\\\N','',item)\n",
    "  return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected from 1133 files\n",
      "0 files had error\n"
     ]
    }
   ],
   "source": [
    "archive = zipfile.ZipFile('../Humor,Hist,Media,Food.zip', 'r')\n",
    "data = []\n",
    "error_files = []\n",
    "file_list = archive.namelist()[1:]\n",
    "for filename in file_list:\n",
    "    try:\n",
    "        with archive.open(filename,'r') as f:\n",
    "            name = str(filename).split(\"/\")[-1]\n",
    "            textlist = []\n",
    "            for line in io.TextIOWrapper(f,'latin-1'):\n",
    "                textlist.append(line)\n",
    "            content = \" \".join(textlist)\n",
    "            data.append({'file':name,'content':content})\n",
    "    except:\n",
    "        error_files.append(str(filename))\n",
    "\n",
    "print(f\"Data collected from {len(data)} files\")\n",
    "print(f\"{len(error_files)} files had error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [05:15<00:00,  3.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for files in tqdm(data):\n",
    "    files['filtered_content'] = filter(files['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting and storing files, wrt to the alphabetical file names\n",
    "\n",
    "data.sort(key =lambda x: x['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping in Pickle File\n",
    "\n",
    "pkl.dump(data,open('./pickle_files/data.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1cbb85fbb159a91b512923319bfb5d4c6de1269d10ca612a23b6e07a28361c8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
