{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/frostrot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing neccesary files\n",
    "\n",
    "import nltk\n",
    "import io\n",
    "import re\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove emojis from the text, if any present\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\" \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"  \n",
    "        u\"\\U0001F1E6-\\U0001F1FF\" \n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all the article connector present in the text\n",
    "\n",
    "def remove_art_connector(text):\n",
    "  article = [\"CAN\",\"IS\",\"HIS\",\"MORE\",\"WHO\",\"ABOUT\",\"THEIR\",\"OUR\",\"HAS\",\"WHO\",\"GET\",\"THEM\",\"WHAT\",\"OUT\",\"FROM\",\"HAVE\",\"HERE\",\"WE\",\"ALL\",\"THERE\",\"TO\",\"ALSO\",\"AND\",\"AS\",\"BUT\",\"YET\",\"YOU\",\"THE\",\"WAS\",\"FOR\",\"ARE\",\"THEY\",\"THIS\",\"THAT\",\"WERE\",\"WITH\",\"YOUR\",\"JUST\",\"WILL\",\"NOT\"]\n",
    "  ans=[]\n",
    "  for word in text:\n",
    "    if word.strip() not in article:\n",
    "      ans.append(word)\n",
    "  return ans\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    x = text.split(\" \")\n",
    "    ps = PorterStemmer()\n",
    "    return \" \".join([ps.stem(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuations from the text\n",
    "\n",
    "def remove_punc(tokens):\n",
    "  table = string.punctuation\n",
    "  ptokens = []\n",
    "  for w in tokens:\n",
    "    if w not in table:\n",
    "      ptokens.append(w)\n",
    "  ptokens = [s for s in ptokens if s]\n",
    "  ptokens = [re.sub(r\"[\\n\\t]+\",\" \",s) for s in ptokens]\n",
    "  return ptokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords, and convert shorted words into there extended forms\n",
    "\n",
    "def stopword(text):\n",
    "  EXTENDED_FORMS = {\"aren't\": 'are not', \"can't\": 'cannot', \"couldn't\": 'could not', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'll\": 'he will', \"he's\": 'he is', \"i'd\": 'i would', \"i'll\": 'i will', \"i'm\": 'i am', \"isn't\": 'is not', \"it's\": 'it is', \"it'll\": 'it will', \"i've\": 'i have', \"let's\": 'let us', \"mightn't\": 'might not', \"mustn't\": 'must not',\"n't\": 'not', \"shan't\": 'shall not', \"she'd\": 'she would', \"she'll\": 'she will', \"she's\": 'she is', \"shouldn't\": 'should not', \"that's\": 'that is', \"there's\": 'there is', \"they'd\": 'they would', \"they'll\": 'they will', \"they're\": 'they are', \"they've\": 'they have', \"we'd\": 'we would', \"we're\": 'we are', \"weren't\": 'were not', \"we've\": 'we have', \"what'll\": 'what will', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"where's\": 'where is', \"who'd\": 'who would', \"who'll\": 'who will', \"who're\": 'who are', \"who's\": 'who is', \"who've\": 'who have', \"won't\": 'will not', \"wouldn't\": 'would not', \"you'd\": 'you would', \"you'll\": 'you will', \"you're\": 'you are', \"you've\": 'you have', \"'re\": ' are', \"wasn't\": 'was not', \"we'll\": 'we will', \"'cause\": 'because', \"could've\": 'could have', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"i'd've\": 'i would have', \"i'll've\": 'i will have', \"it'd\": 'it would', \"it'd've\": 'it would have', \"it'll've\": 'it will have', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd've\": 'she would have', \"she'll've\": 'she will have', \"should've\": 'should have', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so as', \"this's\": 'this is', \"that'd\": 'that would', \"that'd've\": 'that would have', \"there'd\": 'there would', \"there'd've\": 'there would have', \"here's\": 'here is', \"they'd've\": 'they would have', \"they'll've\": 'they will have', \"to've\": 'to have', \"we'd've\": 'we would have', \"we'll've\": 'we will have', \"what'll've\": 'what will have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where've\": 'where have', \"who'll've\": 'who will have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd've\": 'you would have', \"you'll've\": 'you will have'}\n",
    "  x= word_tokenize(text)\n",
    "  for i in range(len(x)):\n",
    "    if x[i] in EXTENDED_FORMS:\n",
    "      x[i] = EXTENDED_FORMS[x[i]]\n",
    "    if x[i] in stopwords.words('english'):\n",
    "      x[i]=''\n",
    "  x=remove_punc(x)\n",
    "  x=remove_art_connector(x)\n",
    "  return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the parsed text, by, converting them into lowercase, removing any tags, extra spaces.\n",
    "\n",
    "def filter(item):\n",
    "  if type(item)==str:\n",
    "    item=item.lower()\n",
    "    item=re.sub('[#@]\\w+\\s*',\"\",item)\n",
    "    item=re.sub(r'\\\\N','',item)\n",
    "    item=remove_emoji(item)\n",
    "    item=stopword(item)\n",
    "    item=stemming(item)\n",
    "  return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected from 1133 files\n",
      "0 files had error\n"
     ]
    }
   ],
   "source": [
    "archive = zipfile.ZipFile('../Humor,Hist,Media,Food.zip', 'r')\n",
    "data = []\n",
    "error_files = []\n",
    "file_list = archive.namelist()[1:]\n",
    "for filename in file_list:\n",
    "    try:\n",
    "        with archive.open(filename,'r') as f:\n",
    "            name = str(filename).split(\"/\")[-1]\n",
    "            textlist = []\n",
    "            for line in io.TextIOWrapper(f,'latin-1'):\n",
    "                textlist.append(line)\n",
    "            content = \" \".join(textlist)\n",
    "            data.append({'file':name,'content':content})\n",
    "    except:\n",
    "        error_files.append(str(filename))\n",
    "\n",
    "print(f\"Data collected from {len(data)} files\")\n",
    "print(f\"{len(error_files)} files had error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [05:44<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "for files in tqdm(data):\n",
    "    files['filtered_content'] = filter(files['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text for file- Date: Wed, 28 Dec 2005 08:16:23 -0700\n",
      " From: Russ Dale \n",
      " To: jason@textfiles.com\n",
      " Subject: APSNET BBS\n",
      " \n",
      " Hello Jason.  I am revisiting my old BBS days and thought I would send\n",
      " along the history of Aurora Colorado's APSNET BBS (303-693-6737).  I\n",
      " wrote the piece for an internal newsletter for the Aurora Public\n",
      " Schools back in 2003.  APSNET is already on the CODEN 303 BBS list @\n",
      " textfiles.com and should be included.  Thanks.  I am also working on a\n",
      " narrative history for my BBS' Vision of Anarchy and The Bottomless\n",
      " Slurpee.  I will send that when it is complete.  Have a great New\n",
      " Year.  --Russ Dale, www.radhole.com\n",
      " \n",
      " The History of APSNET (BBS)\n",
      " \n",
      " The BBS known as APSNET (Aurora Public Schools Net) was the creation\n",
      " of Glenn Blanco, Andy Yuan and Gary Sleap.  It was a Master's Degree\n",
      " project that came online in the fall of 1985.  The BBS (Bulletin Board\n",
      " Service) came online at Columbia Middle School in Aurora.  The\n",
      " equipment used was an Apple IIe with a 20-megabyte hard drive.  It\n",
      " used the GBBS Pro software package, which was the standard for Apple\n",
      " BBS software in the 1980s.  APSNET was named by David Ladek, then an\n",
      " 8th grade student at Columbia.\n",
      " \n",
      " APSNET was online from the fall of 1985 until the fall of 1994.  In\n",
      " its ten-year history, the BBS, with only one phone line (303-693-6737)\n",
      " was averaging over 40 calls per day and well over one thousand calls\n",
      " per month.  After 10 years, the final log of calls was well over one\n",
      " hundred thousand.  During APSNET's tenure, thousands of users\n",
      " logged-in from all over the state of Colorado.  APSNET even received\n",
      " long distance contact from a former user who was in the armed forces\n",
      " serving in Germany.  APSNET was responsible for at least one marriage.\n",
      "  Sysops received thanks from a user who met his partner while\n",
      " exchanging e-mail on the system.\n",
      " \n",
      " APSNET featured electronic mail, bulletin board posting, online USA\n",
      " Today News Service and a full menu of online games and simulations as\n",
      " well as educational downloads.  The APSNET BBS was the first\n",
      " electronic mail service for the Aurora Public Schools.  Most of the\n",
      " users of APSNET were students from schools around the Denver Metro\n",
      " area.  Several teachers used the service on a regular basis.\n",
      " \n",
      " APSNET began with a baud rate of 500 bps in 1985 and progressed\n",
      " through 1200, 2400 and then 9600 bps in 1988.  It was recognized as\n",
      " \"Bulletin Board Service of the Month\" in April of 1988 by the Colorado\n",
      " Apple Computer Users Group.  Schools in APS, Denver Public, Cherry\n",
      " Creek and Douglas County used the service as a training model in\n",
      " middle and high school telecommunications classes.  APSNET inspired\n",
      " the formation of other school Bulletin Board Systems such as the\n",
      " Gatekeeper BBS at Gateway High School\n",
      " \n",
      " APSNET endured as a successful BBS because of direct support for the\n",
      " Aurora Public Schools.  The district provided the phone line and the\n",
      " equipment and software.   Most importantly, the district kept the\n",
      " system free and open for all interested persons using the system.  An\n",
      " eighty-megabyte hard drive was purchased in 1989 increasing the size\n",
      " and speed of the system.\n",
      " \n",
      " In the early 1990s, because of the oncoming use of the internet and\n",
      " the increased demand for communication by computer, APSNET became\n",
      " quickly out of date.  In its ten-year history, APSNET became one of\n",
      " the most utilized and popular systems in the Denver Metro area.  Even\n",
      " to this day, system sysops still receive thanks via electronic mail\n",
      " from former users.  In the late 1990s the name APSNET was revitalized\n",
      " to become the name of the Local Area Network (LAN) of the Aurora\n",
      " Public Schools. \n",
      " \n",
      " .....article by Russ Dale, Denver, Colorado, January 2003\n",
      "\n",
      "Date: Wed, 28 Dec 20\n",
      "Cleaned text for file- date wed 28 dec 2005 08:16:23 -0700 russ dale jason.com subject apsnet bb hello jason revisit old bb day thought would send along histori aurora colorado 's apsnet bb 303-693-6737 wrote piec intern newslett aurora public school back 2003. apsnet alreadi coden 303 bb list textfiles.com includ thank also work narr histori bb vision anarchi bottomless slurpe send complet great new year -- russ dale www.radhole.com histori apsnet bb bb known apsnet aurora public school net creation glenn blanco andi yuan gari sleap master 's degre project came onlin fall 1985. bb bulletin board servic came onlin columbia middl school aurora equip use appl iie 20-megabyt hard drive use gbb pro softwar packag standard appl bb softwar 1980 apsnet name david ladek 8th grade student columbia apsnet onlin fall 1985 fall 1994. ten-year histori bb one phone line 303-693-6737 averag 40 call per day well one thousand call per month 10 year final log call well one hundr thousand apsnet 's tenur thousand user logged-in state colorado apsnet even receiv long distanc contact former user arm forc serv germani apsnet respons least one marriag sysop receiv thank user met partner exchang e-mail system apsnet featur electron mail bulletin board post onlin usa today news servic full menu onlin game simul well educ download apsnet bb first electron mail servic aurora public school user apsnet student school around denver metro area sever teacher use servic regular basi apsnet began baud rate 500 bp 1985 progress 1200 2400 9600 bp 1988. recogn `` bulletin board servic month '' april 1988 colorado appl comput user group school ap denver public cherri creek dougla counti use servic train model middl high school telecommun class apsnet inspir format school bulletin board system gatekeep bb gateway high school apsnet endur success bb direct support aurora public school district provid phone line equip softwar importantli district kept system free open interest person use system eighty-megabyt hard drive purchas 1989 increas size speed system earli 1990 oncom use internet increas demand commun comput apsnet becam quickli date ten-year histori apsnet becam one util popular system denver metro area even day system sysop still receiv thank via electron mail former user late 1990 name apsnet revit becom name local area network lan aurora public school ..... articl russ dale denver colorado januari 2003\n",
      "date wed 28 dec 2005\n"
     ]
    }
   ],
   "source": [
    "for file in data:\n",
    "    print(f\"Text for file- {file['content']}\")\n",
    "    print(file['content'][:20])\n",
    "    print(f\"Cleaned text for file- {file['filtered_content']}\")\n",
    "    print(file['filtered_content'][:20])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting and storing files, wrt to the alphabetical file names\n",
    "\n",
    "data.sort(key =lambda x: x['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping in Pickle File\n",
    "\n",
    "pkl.dump(data,open('./pickle_files/docs.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
